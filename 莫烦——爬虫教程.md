[莫烦学习地址](https://morvanzhou.github.io/tutorials/data-manipulation/scraping/1-01-understand-website/)  
**直接看这个链接里面下面的知识讲解吧，我觉得比自己写的笔记要好多了**
# 1. 网页结构
## 1.1网页基础组成部分
在 HTML 中, 基本上所有的实体内容, 都会有个 tag 来框住它. 而这个被 tag 住的内容, 就可以被展示成不同的形式, 或有不同的功能. 主体的 tag 分成两部分,`header`和`body`. 在`header`中, 存放这一些网页的网页的元信息, 比如说`title`, 这些信息是不会被显示到你看到的网页中的. 这些信息大多数时候是给浏览器看, 或者是给搜索引擎的爬虫看。
```python
<head>
	<meta charset="UTF-8">
	<title>Scraping tutorial 1 | 莫烦Python</title>
	<link rel="icon" href="https://morvanzhou.github.io/static/img/description/tab_icon.png">
</head>
```
HTML 的第二大块是`body`, 这个部分才是你看到的网页信息. 网页中的 heading, 视频, 图片和文字等都存放在这里. 这里的`<h1>``</h1>` tag 就是主标题, 我们看到呈现出来的效果就是大一号的文字.`<p>``</p>`里面的文字就是一个段落.`<a>``</a>`里面都是一些链接. 所以很多情况, 东西都是放在这些 tag 中的.
```python
<body>
    <h1>爬虫测试1</h1>
    <p>
        这是一个在 <a href="https://morvanzhou.github.io/">莫烦Python</a>
        <a href="https://morvanzhou.github.io/tutorials/scraping">爬虫教程</a> 中的简单测试.
    </p>
</body>
```
爬虫想要做的就是根据这些 tag 来找到合适的信息.

## 1.2用Python登录网页

# 2. BeautifulSoup4
使用BeautifulSoup4爬取上次爬取的那个基本网页
```python
from bs4 import BeautifulSoup
from urllib.request import urlopen

# if has Chinese, apply decode()
html = urlopen("https://morvanzhou.github.io/static/scraping/basic-structure.html").read().decode('utf-8')
print(html)
```
回顾一下, 每张网页中, 都有两大块, 一个是`<head>`, 一个是`<body>`, 我们等会用 BeautifulSoup 来找到 body 中的段落`<p>`和所有链接`<a>`.
以下为网页源码
```html
<!DOCTYPE html>
<html lang="cn">
<head>
	<meta charset="UTF-8">
	<title>Scraping tutorial 1 | 莫烦Python</title>
	<link rel="icon" href="https://morvanzhou.github.io/static/img/description/tab_icon.png">
</head>
<body>
	<h1>爬虫测试1</h1>
	<p>
		这是一个在 <a href="https://morvanzhou.github.io/">莫烦Python</a>
		<a href="https://morvanzhou.github.io/tutorials/scraping">爬虫教程</a> 中的简单测试.
	</p>

</body>
</html>
```
读取这个网页信息, 我们将要加载进`BeautifulSoup`, 以`lxml`的这种形式加载. 除了`lxml`, 其实还有很多形式的解析器, 不过大家都推荐使用`lxml`的形式. 然后`soup`里面就有着这个`HTML`的所有信息. 如果你要输出 `<h1>`标题, 可以就直接`soup.h1`.
```python
soup = BeautifulSoup(html, features='lxml')
print(soup.h1)

"""
<h1>爬虫测试1</h1>
"""

print('\n', soup.p)

"""
<p>
		这是一个在 <a href="https://morvanzhou.github.io/">莫烦Python</a>
<a href="https://morvanzhou.github.io/tutorials/scraping">爬虫教程</a> 中的简单测试.
	</p>
"""
```
如果网页中有过个同样的`tag`,比如链接`<a>`, 我们可以使用`find_all()`来找到所有的选项. 因为我们真正的`link`不是在`<a>`中间`</a>`, 而是在`<a href="link">`里面, 也可以看做是`<a>`的一个属性. 我们能用像`Python`字典的形式, 用`key`来读取`l["href"]`.
```python
"""
<a href="https://morvanzhou.github.io/tutorials/scraping">爬虫教程</a>
"""

all_href = soup.find_all('a')
all_href = [l['href'] for l in all_href]
print('\n', all_href)

# ['https://morvanzhou.github.io/', 'https://morvanzhou.github.io/tutorials/scraping']
```
注：
* 对`print i for i in [1,2,3]`的理解  
`print i for i in [1,2,3]`相当于以下的代码
```python
def f(x): print x
[f(i) for i in [1,2,3]]
```
`(f(i)...)`在这不起作用，因为这只会创建一个生成器,`f()`如果您迭代它就会调用它，列表`[]`实际上就是调用`f()`
* 对于l['href']的理解
此处可以用字典中的键值对来理解，将`href`看做键key，那么l['href']就相当于其对应的键值对

**如何查到网页对应内容在html源码中的位置**：选中那段文字，然后右键，检查即可得到，这个方法很重要，不然去源码里面一点一点的找很傻逼
爬百度百科中的一些小总结
```python
html = urlopen(url).read().decode('utf-8')
soup = BeautifulSoup(html, 'lxml')
print(soup.find('h1').get_text(), '     url', his[-1])
```
`find`可以用于查找`beautifulsoup`容器中的`tag`，然后再利用`get_text()`获得这部分`tag`的内容。  
在python中单引号和双引号地位相同，都既可以表示字符，也可以表示字符串